{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9uXoN6O837FnAUDgehCCe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DileshworiJ/Data-maipulation-and-Visualization-R/blob/master/artificial_neural_network_udemy_lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCkXJvw_g1Vi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#perceptron Model\n",
        "\n",
        "To begin understanding deep learning, we will build up our model abstractions:\n",
        "*   single biological neuron\n",
        "*   Perceptron\n",
        "*   Multilayer Perceptron Model\n",
        "*   Deep Learning Neural Network\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E6zElHKehJyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mathematical concepts\n",
        "\n",
        "\n",
        "*   Activation Functions\n",
        "*   Gradient Descent\n",
        "*   BackPropagation\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-0wCth4Eh2Lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Neural network\n",
        "single perceptron to create a multilayer perceptron model => the basic neural network.\n",
        "\n",
        "The idea of activation function.\n",
        "  \n",
        "= Layers of perceptrons \n",
        "= perceptron and neuron interchangeable\n",
        "\n",
        "= interactions and relationships between features.\n",
        "\n",
        "= first layer is the input layer\n",
        "=last layer is the output layer > last layer can be more than one neuron\n",
        "= any layer between input layer and output layers are the hidden layers"
      ],
      "metadata": {
        "id": "aSaHB7YPnL10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "= neural network become deep neural networks if they contain 2 or more hidden layers\\\n",
        "\n",
        "##convex continuous function can be be approximated by Neural networks\n",
        "### Universal Approximation Theorem"
      ],
      "metadata": {
        "id": "U1nm32bHoQ__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# not just simple sum function with weight\n",
        "\n",
        "@@@Activation function\n",
        "we may want to add constraint on the function...like in a classification tasks, it would be useful to have all outputs fall between 0 and 1.\n",
        "\n",
        "*   These values can then present probability assignments for each class.\n",
        "\n",
        "\n",
        "\n",
        "*   using activation functions to set boundaries to output values from the neuron\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_63h5JWIpq5U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HHBXI4aYscS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Activation Functions\n",
        "\n",
        "earlier:\n",
        "  x*w + b \n",
        "\n",
        "      b= offset value, making x*w have to reach a certain threshold before having an effect\n",
        "\n",
        "       w = how much weight or strength to give the incoming input \n",
        "\n",
        "\n",
        "*   for example if b=-10, then the effects of x*w won't really start to overcome the bias until their product surpasses 10\n",
        "\n",
        "# next we want to set boundaries for the overall output value of:\n",
        "  x*w + b\n",
        "\n",
        "  we can state:\n",
        "   z = x*w + b\n",
        "\n",
        "   And then pass z through some activation function to limit its value\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     \n"
      ],
      "metadata": {
        "id": "8jwUQ872qfMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#common activation functions\n",
        " @@@ in case of binary classification problem"
      ],
      "metadata": {
        "id": "FiUrl0yvtf7b"
      }
    }
  ]
}